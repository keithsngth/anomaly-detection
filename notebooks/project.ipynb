{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "\n",
    "## Table of Contents\n",
    "1. [Libraries](#1)\n",
    "2. [Section 2](#2)\n",
    "3. [Section 3](#3)\n",
    "4. [Section 4](#4)\n",
    "5. [Conclusion](#5)\n",
    "\n",
    "__Introduction__  \n",
    "This project focuses on ...\n",
    "\n",
    "__Datasets Used__  \n",
    "- [Dataset 1](URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set-Up <a id = '1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/02 00:25:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with proper configuration\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"AnomalyDetection\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")\n",
    "    .config(\n",
    "        \"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Set log level to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA <a id = '2'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: bank-additional-full-weka.filters.unsupervised.attribute.RemoveType-Tnumeric\n",
      "\tjob's type is nominal, range is ('housemaid', 'services', 'admin.', 'blue-collar', 'technician', 'retired', 'management', 'unemployed', 'self-employed', 'unknown', 'entrepreneur', 'student')\n",
      "\tmarital's type is nominal, range is ('married', 'single', 'divorced', 'unknown')\n",
      "\teducation's type is nominal, range is ('basic.4y', 'high.school', 'basic.6y', 'basic.9y', 'professional.course', 'unknown', 'university.degree', 'illiterate')\n",
      "\tdefault's type is nominal, range is ('no', 'unknown', 'yes')\n",
      "\thousing's type is nominal, range is ('no', 'yes', 'unknown')\n",
      "\tloan's type is nominal, range is ('no', 'yes', 'unknown')\n",
      "\tcontact's type is nominal, range is ('telephone', 'cellular')\n",
      "\tmonth's type is nominal, range is ('may', 'jun', 'jul', 'aug', 'oct', 'nov', 'dec', 'mar', 'apr', 'sep')\n",
      "\tday_of_week's type is nominal, range is ('mon', 'tue', 'wed', 'thu', 'fri')\n",
      "\tpoutcome's type is nominal, range is ('nonexistent', 'failure', 'success')\n",
      "\ty's type is nominal, range is ('yes', 'no')\n",
      "\n",
      "root\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n",
      "+---------+-------+-----------+-------+-------+----+---------+-----+-----------+-----------+---+\n",
      "|      job|marital|  education|default|housing|loan|  contact|month|day_of_week|   poutcome|  y|\n",
      "+---------+-------+-----------+-------+-------+----+---------+-----+-----------+-----------+---+\n",
      "|housemaid|married|   basic.4y|     no|     no|  no|telephone|  may|        mon|nonexistent| no|\n",
      "| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|nonexistent| no|\n",
      "| services|married|high.school|     no|    yes|  no|telephone|  may|        mon|nonexistent| no|\n",
      "|   admin.|married|   basic.6y|     no|     no|  no|telephone|  may|        mon|nonexistent| no|\n",
      "| services|married|high.school|     no|     no| yes|telephone|  may|        mon|nonexistent| no|\n",
      "+---------+-------+-----------+-------+-------+----+---------+-----+-----------+-----------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41188"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read arff file\n",
    "data, meta = arff.loadarff(\"../data/bank-additional-ful-nominal.arff\")\n",
    "df = pd.DataFrame(data)\n",
    "df = df.applymap(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n",
    "\n",
    "# Examine metadata\n",
    "print(meta)\n",
    "\n",
    "# Convert to spark df\n",
    "bank_df = spark.createDataFrame(df)\n",
    "bank_df.createOrReplaceTempView(\"BANK\")\n",
    "\n",
    "bank_df.printSchema()\n",
    "spark.sql(\"SELECT * FROM BANK LIMIT 5\").show()\n",
    "bank_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
